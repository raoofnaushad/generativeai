{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Evaluation Baseline - using LlamaIndex\n",
    "LlamaIndex provides some basic evaluation of query engines! We can setup an evaluator that will measure both hallucinations, as well as if the query was actually answered!\n",
    "\n",
    "This is provided by two main evaluations:\n",
    "\n",
    "- `ResponseSourceEvaluator` - uses an LLM to decide if the response is similar enough to the sources -- a good measure for hallunication detection!\n",
    "- `QueryResponseEvaluator` - uses an LLM to decide if a response is similar enough to the original query -- a good measure for checking if the query was answered!\n",
    "\n",
    "You may have noticed that we are using an LLM for this task. That means we will want to pick a powerful LLM, like GPT-4 or Claude-2.\n",
    "\n",
    "Lastly, using these methods, we can also use the LLM to generate syntheic questions to evaluate with!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the baseline query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.markdown_docs_reader import MarkdownDocsReader\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "def load_markdown_docs(filepath):\n",
    "    \"\"\"Load markdown docs from a directory, excluding all other file types.\"\"\"\n",
    "    loader = SimpleDirectoryReader(\n",
    "        input_dir=filepath, \n",
    "        required_exts=[\".md\"],\n",
    "        file_extractor={\".md\": MarkdownDocsReader()},\n",
    "        recursive=True\n",
    "    )\n",
    "\n",
    "    documents = loader.load_data()\n",
    "\n",
    "\n",
    "    ### All these metadata will be useful for embeddings for retrieval. But when it is passed to LLM it is not required.\n",
    "    # exclude some metadata from the LLM\n",
    "    for doc in documents:\n",
    "        doc.excluded_llm_metadata_keys = [\"File Name\", \"Content Type\", \"Header Path\"]\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load our documents from each folder.\n",
    "\n",
    "base_path = '/Users/raoofmac/Documents/coding/learning/genai'\n",
    "\n",
    "getting_started_docs =  load_markdown_docs(os.path.join(base_path, \"docs/getting_started\"))\n",
    "community_docs =  load_markdown_docs(os.path.join(base_path, \"docs/community\"))\n",
    "data_docs =  load_markdown_docs(os.path.join(base_path, \"docs/core_modules/data_modules\"))\n",
    "agent_docs =  load_markdown_docs(os.path.join(base_path, \"docs/core_modules/agent_modules\"))\n",
    "model_docs =  load_markdown_docs(os.path.join(base_path, \"docs/core_modules/model_modules\"))\n",
    "query_docs =  load_markdown_docs(os.path.join(base_path, \"docs/core_modules/query_modules\"))\n",
    "supporting_docs =  load_markdown_docs(os.path.join(base_path, \"docs/core_modules/supporting_modules\"))\n",
    "tutorial_docs =  load_markdown_docs(os.path.join(base_path, \"docs/end_to_end_tutorials\"))\n",
    "contributing_docs =  load_markdown_docs(os.path.join(base_path, \"docs/development\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.core.indices.service_context import ServiceContext, set_global_service_context\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "\n",
    "## Setting up the models for llm and embedding model\n",
    "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "# Settings.embed_model = OpenAIEmbedding(\n",
    "#     model=\"text-embedding-3-small\", embed_batch_size=100\n",
    "# )\n",
    "# # maximum input size to the LLM\n",
    "# Settings.context_window = 4096\n",
    "# # number of tokens to leave room for the LLM to generate\n",
    "# Settings.num_output = 256\n",
    "# Settings.text_splitter = SentenceSplitter(chunk_size=1024)\n",
    "# Settings.chunk_size = 512\n",
    "# Settings.chunk_overlap = 20\n",
    "# Settings.transformations = [SentenceSplitter(chunk_size=1024)]\n",
    "## Tokenizer\n",
    "# import tiktoken\n",
    "# Settings.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\").encode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, StorageContext, load_index_from_storage\n",
    "\n",
    "try:\n",
    "    getting_started_index = load_index_from_storage(StorageContext.from_defaults(persist_dir=\"./storage/getting_started_index\"))\n",
    "    community_index = load_index_from_storage(StorageContext.from_defaults(persist_dir=\"./storage/community_index\"))\n",
    "    data_index = load_index_from_storage(StorageContext.from_defaults(persist_dir=\"./storage/data_index\"))\n",
    "    agent_index = load_index_from_storage(StorageContext.from_defaults(persist_dir=\"./storage/agent_index\"))\n",
    "    model_index = load_index_from_storage(StorageContext.from_defaults(persist_dir=\"./storage/model_index\"))\n",
    "    query_index = load_index_from_storage(StorageContext.from_defaults(persist_dir=\"./storage/query_index\"))\n",
    "    supporting_index = load_index_from_storage(StorageContext.from_defaults(persist_dir=\"./storage/supporting_index\"))\n",
    "    tutorials_index = load_index_from_storage(StorageContext.from_defaults(persist_dir=\"./storage/tutorials_index\"))\n",
    "    contributing_index = load_index_from_storage(StorageContext.from_defaults(persist_dir=\"./storage/contributing_index\"))\n",
    "except:\n",
    "    print(\"Index Not Found! Creating Index\")\n",
    "    getting_started_index = VectorStoreIndex.from_documents(getting_started_docs)\n",
    "    getting_started_index.storage_context.persist(persist_dir=\"./storage/getting_started_index\")\n",
    "\n",
    "    community_index = VectorStoreIndex.from_documents(community_docs)\n",
    "    community_index.storage_context.persist(persist_dir=\"./storage/community_index\")\n",
    "\n",
    "    data_index = VectorStoreIndex.from_documents(data_docs)\n",
    "    data_index.storage_context.persist(persist_dir=\"./storage/data_index\")\n",
    "\n",
    "    agent_index = VectorStoreIndex.from_documents(agent_docs)\n",
    "    agent_index.storage_context.persist(persist_dir=\"./storage/agent_index\")\n",
    "\n",
    "    model_index = VectorStoreIndex.from_documents(model_docs)\n",
    "    model_index.storage_context.persist(persist_dir=\"./storage/model_index\")\n",
    "\n",
    "    query_index = VectorStoreIndex.from_documents(query_docs)\n",
    "    query_index.storage_context.persist(persist_dir=\"./storage/query_index\")\n",
    "\n",
    "    supporting_index = VectorStoreIndex.from_documents(supporting_docs)\n",
    "    supporting_index.storage_context.persist(persist_dir=\"./storage/supporting_index\")\n",
    "\n",
    "    tutorials_index = VectorStoreIndex.from_documents(tutorial_docs)\n",
    "    tutorials_index.storage_context.persist(persist_dir=\"./storage/tutorials_index\")\n",
    "\n",
    "    contributing_index = VectorStoreIndex.from_documents(contributing_docs)\n",
    "    contributing_index.storage_context.persist(persist_dir=\"./storage/contributing_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Query Engine Tools\n",
    "\n",
    "Since we have a lot of indices, we can create a query engine tool for each and then use them in a single query engine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "# create a query engine tool for each folder\n",
    "getting_started_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=getting_started_index.as_query_engine(), \n",
    "    name=\"Getting Started\", \n",
    "    description=\"Useful for answering questions about installing and running llama index, as well as basic explanations of how llama index works.\"\n",
    ")\n",
    "\n",
    "community_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=community_index.as_query_engine(),\n",
    "    name=\"Community\",\n",
    "    description=\"Useful for answering questions about integrations and other apps built by the community.\"\n",
    ")\n",
    "\n",
    "data_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=data_index.as_query_engine(),\n",
    "    name=\"Data Modules\",\n",
    "    description=\"Useful for answering questions about data loaders, documents, nodes, and index structures.\"\n",
    ")\n",
    "\n",
    "agent_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=agent_index.as_query_engine(),\n",
    "    name=\"Agent Modules\",\n",
    "    description=\"Useful for answering questions about data agents, agent configurations, and tools.\"\n",
    ")\n",
    "\n",
    "model_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=model_index.as_query_engine(),\n",
    "    name=\"Model Modules\",\n",
    "    description=\"Useful for answering questions about using and configuring LLMs, embedding modles, and prompts.\"\n",
    ")\n",
    "\n",
    "query_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=query_index.as_query_engine(),\n",
    "    name=\"Query Modules\",\n",
    "    description=\"Useful for answering questions about query engines, query configurations, and using various parts of the query engine pipeline.\"\n",
    ")\n",
    "\n",
    "supporting_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=supporting_index.as_query_engine(),\n",
    "    name=\"Supporting Modules\",\n",
    "    description=\"Useful for answering questions about supporting modules, such as callbacks, service context, and avaluation.\"\n",
    ")\n",
    "\n",
    "tutorials_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=tutorials_index.as_query_engine(),\n",
    "    name=\"Tutorials\",\n",
    "    description=\"Useful for answering questions about end-to-end tutorials and giving examples of specific use-cases.\"\n",
    ")\n",
    "\n",
    "contributing_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=contributing_index.as_query_engine(),\n",
    "    name=\"Contributing\",\n",
    "    description=\"Useful for answering questions about contributing to llama index, including how to contribute to the codebase and how to build documentation.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from llama_index.core.query_engine import SubQuestionQueryEngine\n",
    "from llama_index.core.response_synthesizers import get_response_synthesizer\n",
    "\n",
    "query_engine = SubQuestionQueryEngine.from_defaults(\n",
    "    query_engine_tools=[\n",
    "        getting_started_tool,\n",
    "        community_tool,\n",
    "        data_tool,\n",
    "        agent_tool,\n",
    "        model_tool,\n",
    "        query_tool,\n",
    "        supporting_tool,\n",
    "        tutorials_tool,\n",
    "        contributing_tool\n",
    "    ],\n",
    "    # enable this for streaming\n",
    "    # response_synthesizer=get_response_synthesizer(streaming=True),\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To install llama index, you can follow these steps:\n",
      "1. Clone the repository using Git: `git clone https://github.com/jerryjliu/llama_index.git`.\n",
      "2. If you want to do an editable install, run `pip install -e .`.\n",
      "3. If you want to install optional dependencies and dependencies used for development, run `pip install -r requirements.txt`.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"How do I install llama index?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "documents = SimpleDirectoryReader(\"../../docs\", recursive=True, required_exts=[\".md\"]).load_data()\n",
    "\n",
    "all_text = \"\"\n",
    "\n",
    "for doc in documents:\n",
    "    all_text += doc.text\n",
    "\n",
    "giant_document = Document(text=all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z6/yb_00mr541q1fbtlygsbgk6m0000gn/T/ipykernel_81074/554714625.py:10: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  gpt4_service_context = ServiceContext.from_defaults(llm=OpenAI(llm=\"gpt-4\", temperature=0))\n",
      "/Users/raoofmac/Documents/coding/learning/genai/venv/lib/python3.9/site-packages/llama_index/core/evaluation/dataset_generation.py:212: DeprecationWarning: Call to deprecated class DatasetGenerator. (Deprecated in favor of `RagDatasetGenerator` which should be used instead.)\n",
      "  return cls(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 3 questions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raoofmac/Documents/coding/learning/genai/venv/lib/python3.9/site-packages/llama_index/core/evaluation/dataset_generation.py:309: DeprecationWarning: Call to deprecated class QueryResponseDataset. (Deprecated in favor of `LabelledRagDataset` which should be used instead.)\n",
      "  return QueryResponseDataset(queries=queries, responses=responses_dict)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "from llama_index.core import ServiceContext\n",
    "from llama_index.core.prompts import Prompt\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.evaluation import DatasetGenerator\n",
    "\n",
    "gpt4_service_context = ServiceContext.from_defaults(llm=OpenAI(llm=\"gpt-4\", temperature=0))\n",
    "\n",
    "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "\n",
    "question_dataset = []\n",
    "if os.path.exists(\"question_dataset.txt\"):\n",
    "    with open(\"question_dataset.txt\", \"r\") as f:\n",
    "        for line in f:\n",
    "            question_dataset.append(line.strip())\n",
    "else:\n",
    "    # generate questions\n",
    "    data_generator = DatasetGenerator.from_documents(\n",
    "        [giant_document],\n",
    "        text_question_template=Prompt(\n",
    "            \"A sample from the LlamaIndex documentation is below.\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"{context_str}\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"Using the documentation sample, carefully follow the instructions below:\\n\"\n",
    "            \"{query_str}\"\n",
    "        ),\n",
    "        question_gen_query=(\n",
    "            \"You are an evaluator for a search pipeline. Your task is to write a single question \"\n",
    "            \"using the provided documentation sample above to test the search pipeline. The question should \"\n",
    "            \"reference specific names, functions, and terms. Restrict the question to the \"\n",
    "            \"context information provided.\\n\"\n",
    "            \"Question: \"\n",
    "        ),\n",
    "        # set this to be low, so we can generate more questions\n",
    "        service_context=gpt4_service_context\n",
    "    )\n",
    "    generated_questions = data_generator.generate_questions_from_nodes(num=3)\n",
    "\n",
    "    # randomly pick 40 questions from each dataset\n",
    "    # generated_questions = random.sample(generated_questions, )\n",
    "    question_dataset.extend(generated_questions)\n",
    "\n",
    "    print(f\"Generated {len(question_dataset)} questions.\")\n",
    "\n",
    "    # save the questions!\n",
    "    with open(\"question_dataset.txt\", \"w\") as f:\n",
    "        for question in question_dataset:\n",
    "            f.write(f\"{question.strip()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating Response for Hallucination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from llama_index.core import Response\n",
    "\n",
    "def evaluate_query_engine(evaluator, query_engine, questions):\n",
    "    async def run_query(query_engine, q):\n",
    "        try:\n",
    "            return await query_engine.aquery(q)\n",
    "        except:\n",
    "            return Response(response=\"Error, query failed.\")\n",
    "\n",
    "    total_correct = 0\n",
    "    all_results = []\n",
    "    for batch_size in range(0, len(questions), 3):\n",
    "        batch_qs = questions[batch_size:batch_size+5]\n",
    "\n",
    "        tasks = [run_query(query_engine, q) for q in batch_qs]\n",
    "        responses = asyncio.run(asyncio.gather(*tasks))\n",
    "        print(f\"finished batch {(batch_size // 5) + 1} out of {len(questions) // 5}\")\n",
    "\n",
    "        for response in responses:\n",
    "            eval_result = 1 if \"YES\" in evaluator.evaluate(response) else 0\n",
    "            total_correct += eval_result\n",
    "            all_results.append(eval_result)\n",
    "        \n",
    "        # helps avoid rate limits\n",
    "        time.sleep(1)\n",
    "\n",
    "    return total_correct, all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import ResponseEvaluator\n",
    "\n",
    "evaluator = ResponseEvaluator(service_context=gpt4_service_context)\n",
    "total_correct, all_results = evaluate_query_engine(evaluator, query_engine, question_dataset)\n",
    "\n",
    "print(f\"Hallucination? Scored {total_correct} out of {len(question_dataset)} questions correctly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m hallucinated_questions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(question_dataset)[np\u001b[38;5;241m.\u001b[39marray(\u001b[43mall_results\u001b[49m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(hallucinated_questions)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_results' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "hallucinated_questions = np.array(question_dataset)[np.array(all_results) == 0]\n",
    "print(hallucinated_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work on the rest!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
